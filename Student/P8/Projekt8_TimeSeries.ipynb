{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wczytanie datasetu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "CLASS_NAMES = [\"Abnormal\", \"Normal\"]\n",
    "\n",
    "with open(\"Wafer_TRAIN.txt\", \"r\") as f:\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for line in f.readlines():\n",
    "        line = line.strip().split(\"  \")\n",
    "        class_id = int(float(line[0]))\n",
    "        class_id = max(class_id, 0)\n",
    "        data = np.array(line[1:], dtype=np.float64)\n",
    "        X_train.append(data)\n",
    "        y_train.append(class_id)\n",
    "\n",
    "with open(\"Wafer_TEST.txt\", \"r\") as f:\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    for line in f.readlines():\n",
    "        line = line.strip().split(\"  \")\n",
    "        class_id = int(float(line[0]))\n",
    "        class_id = max(class_id, 0)\n",
    "        data = np.array(line[1:], dtype=np.float64)\n",
    "        X_test.append(data)\n",
    "        y_test.append(class_id)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wizualizacja przebiegów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig,axes = plt.subplots(nrows=len(CLASS_NAMES),figsize=(10,5))\n",
    "for i,ax in enumerate(axes):\n",
    "    for j in range(5):\n",
    "        index = np.where(y_test == i)[0][j]\n",
    "        ax.plot(np.array(X_test[index], dtype=float), linewidth=1, alpha=0.5)\n",
    "    ax.set_title(f\"{CLASS_NAMES[i]}\")\n",
    "    ax.set_xlabel(\"Samples [-]\")\n",
    "    ax.set_ylabel(\"Sensor Value [-]\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"class_values = {}\n",
    "class_stats = {}\n",
    "for class_id in range(len(CLASS_NAMES)):\n",
    "    class_values[class_id] = []\n",
    "    class_stats[class_id] = {\n",
    "        'mean': [],\n",
    "        'std': [],\n",
    "        'max': [],\n",
    "        'min': []\n",
    "    }\n",
    "    for index in np.where(y_train == class_id)[0]:\n",
    "        class_values[class_id] += X_train[index].tolist()\n",
    "\n",
    "        class_stats[class_id]['mean'].append(np.mean(X_train[index]))\n",
    "        class_stats[class_id]['std'].append(np.std(X_train[index]))\n",
    "        class_stats[class_id]['max'].append(np.max(X_train[index]))\n",
    "        class_stats[class_id]['min'].append(np.min(X_train[index]))\n",
    "    for key in class_stats[class_id]:\n",
    "        class_stats[class_id][key] = np.mean(class_stats[class_id][key]), np.std(class_stats[class_id][key])\n",
    "\n",
    "fig,axes = plt.subplots(nrows=len(CLASS_NAMES),figsize=(10,15))\n",
    "\n",
    "for i,ax in enumerate(axes):\n",
    "    ax.hist(class_values[i], bins=50, alpha=0.75, color='royalblue', edgecolor='black')\n",
    "    ax.set_title(f\"{CLASS_NAMES[i]}\")\n",
    "    ax.set_xlabel(\"Power [W]\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for class_id in range(len(CLASS_NAMES)):\n",
    "    print(f\"Class: {CLASS_NAMES[class_id]}\")\n",
    "    print(f\"\\tMean: {class_stats[class_id]['mean'][0]:.3f} ({class_stats[class_id]['mean'][1]:.3f})\")\n",
    "    print(f\"\\tStd: {class_stats[class_id]['std'][0]:.3f} ({class_stats[class_id]['std'][1]:.3f})\")\n",
    "    print(f\"\\tMax: {class_stats[class_id]['max'][0]:.3f} ({class_stats[class_id]['max'][1]:.3f})\")\n",
    "    print(f\"\\tMin: {class_stats[class_id]['min'][0]:.3f} ({class_stats[class_id]['min'][1]:.3f})\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def trans_func(input_series: np.ndarray) -> np.ndarray:\n",
    "    return input_series # Dummy\n",
    "\n",
    "transformer = FunctionTransformer(trans_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Klasyfikacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stworzenie pipeline'u\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "pipeline_SVM = Pipeline([\n",
    "    ('feature_extractor', transformer),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', SVC(kernel='rbf'))\n",
    "])\n",
    "\n",
    "pipeline_RF = Pipeline([\n",
    "    ('feature_extractor', transformer),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "pipeline_KNN = Pipeline([\n",
    "    ('feature_extractor', transformer),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', KNeighborsClassifier())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_SVM.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_RF.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_KNN.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from seaborn import heatmap\n",
    "\n",
    "preds_svm = pipeline_SVM.predict(X_test)\n",
    "\n",
    "report_svm = classification_report(y_test, preds_svm, target_names=CLASS_NAMES, zero_division=0, digits=3)\n",
    "print(report_svm)\n",
    "\n",
    "confusion_matrix_svm = confusion_matrix(y_test, preds_svm)\n",
    "confusion_matrix_svm = confusion_matrix_svm / confusion_matrix_svm[:, :].sum(axis=1)[:, np.newaxis]\n",
    "ax = heatmap(confusion_matrix_svm, annot=True, xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES, cmap='Blues', fmt=\".3f\")\n",
    "ax.set_xlabel(\"Predicted\") \n",
    "ax.set_ylabel(\"Ground truth\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_rf = pipeline_RF.predict(X_test)\n",
    "\n",
    "report_rf = classification_report(y_test, preds_rf, target_names=CLASS_NAMES, zero_division=0, digits=3)\n",
    "print(report_rf)\n",
    "\n",
    "confusion_matrix_rf = confusion_matrix(y_test, preds_rf)\n",
    "confusion_matrix_rf = confusion_matrix_rf / confusion_matrix_rf[:, :].sum(axis=1)[:, np.newaxis]\n",
    "ax = heatmap(confusion_matrix_rf, annot=True, xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES, cmap='Blues', fmt=\".3f\")\n",
    "ax.set_xlabel(\"Predicted\") \n",
    "ax.set_ylabel(\"Ground truth\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_knn = pipeline_KNN.predict(X_test)\n",
    "\n",
    "report_knn = classification_report(y_test, preds_knn, target_names=CLASS_NAMES, zero_division=0, digits=3)\n",
    "print(report_knn)\n",
    "\n",
    "confusion_matrix_knn = confusion_matrix(y_test, preds_knn)\n",
    "confusion_matrix_knn = confusion_matrix_knn / confusion_matrix_knn[:, :].sum(axis=1)[:, np.newaxis]\n",
    "ax = heatmap(confusion_matrix_knn, annot=True, xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES, cmap='Blues', fmt=\".3f\")\n",
    "ax.set_xlabel(\"Predicted\") \n",
    "ax.set_ylabel(\"Ground truth\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wykorzystanie CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stworzenie datasetu i dataloadera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        # Add channel dimension\n",
    "        X = np.expand_dims(X, axis=1) # N x C x L, with C=1\n",
    "        # Standarize\n",
    "        X = (X - np.mean(X)) / np.std(X)\n",
    "\n",
    "        self.X = torch.tensor(X.astype(np.float32))\n",
    "        self.y = torch.tensor(y.astype(np.int64))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "    def get_class_weights(self):\n",
    "        class_sample_count = np.unique(self.y.numpy(), return_counts=True)[1]\n",
    "        weight = (np.sum(class_sample_count) / class_sample_count).astype(np.float32)\n",
    "        return torch.from_numpy(weight)\n",
    "\n",
    "class CustomDataloader(DataLoader):\n",
    "    def __init__(self, dataset, batch_size, shuffle, augment: bool = False):\n",
    "        super().__init__(dataset, batch_size, shuffle)\n",
    "        self.augment = augment\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in super().__iter__():\n",
    "            if self.augment:\n",
    "                batch = self.augment_batch(batch)\n",
    "            yield batch\n",
    "\n",
    "    def augment_batch(self, batch):\n",
    "        data,labels = batch\n",
    "        for i in range(len(data)):\n",
    "            # Add offset\n",
    "            data[i] = data[i] + torch.rand(1)*0.1\n",
    "\n",
    "        return [data,labels]\n",
    "\n",
    "# Dataloader\n",
    "dataset_train = CustomDataset(X_train, y_train)\n",
    "dataloader_train = CustomDataloader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, augment=True)\n",
    "\n",
    "dataset_test = CustomDataset(X_test, y_test)\n",
    "dataloader_test = CustomDataloader(dataset_test, batch_size=BATCH_SIZE, shuffle=False, augment=False)\n",
    "\n",
    "print(dataset_train.get_class_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definicja własnej sieci neuronowej\n",
    "\n",
    "https://journals.sagepub.com/doi/full/10.3233/ICA-200617"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install enchanter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_class: int, n_channels_in: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        CHANNELS_0 = 32\n",
    "        CHANNELS_1 = 64\n",
    "        CHANNELS_2 = 128\n",
    "        NEURONS_0 = 512\n",
    "        NEURONS_1 = 128\n",
    "\n",
    "        self.conv0 = nn.Sequential(\n",
    "            nn.Conv1d(n_channels_in, out_channels=CHANNELS_0, kernel_size=7, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(CHANNELS_0, out_channels=CHANNELS_1, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(CHANNELS_1, out_channels=CHANNELS_2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "        )\n",
    "        \n",
    "        self.fc0 = nn.Sequential(\n",
    "            nn.Linear(CHANNELS_2, NEURONS_0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(NEURONS_0, NEURONS_1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(NEURONS_1, n_class),\n",
    "            #nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        \n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        out = self.conv0(x)\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        # Global max pooling\n",
    "        out = torch.max(out, dim=2).values\n",
    "\n",
    "\n",
    "        out = self.fc0(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def print_data_shapes(self, input_length: int) -> None:\n",
    "\n",
    "        x = torch.randn(1, 1, input_length).to(next(self.parameters()).device)\n",
    "        print(f\"Input shape: {x.shape[1:]}\")\n",
    "\n",
    "        out = self.conv0(x)\n",
    "        print(f\"Conv0 shape: {out.shape[1:]}\")\n",
    "\n",
    "        out = self.conv1(out)\n",
    "        print(f\"Conv1 shape: {out.shape[1:]}\")\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        print(f\"Conv2 shape: {out.shape[1:]}\")\n",
    "\n",
    "        out = torch.max(out, dim=2).values\n",
    "        print(f\"Global max pooling shape: {out.shape}\")\n",
    "\n",
    "        out = self.fc0(out)\n",
    "        print(f\"FC0 shape: {out.shape[1:]}\")\n",
    "\n",
    "        out = self.fc1(out)\n",
    "        print(f\"FC1 shape: {out.shape[1:]}\")\n",
    "\n",
    "        out = self.fc2(out)\n",
    "        print(f\"FC2 shape: {out.shape[1:]}\")\n",
    "    \n",
    "    def get_conv_embedding(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        out = self.conv0(x)\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        # Global max pooling\n",
    "        out = torch.max(out, dim=2).values\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def get_fc_embedding(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.conv0(x)\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        # Global max pooling\n",
    "        out = torch.max(out, dim=2).values\n",
    "\n",
    "\n",
    "        out = self.fc0(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc1(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = self(X).argmax(dim=1)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda import is_available\n",
    "\n",
    "iscuda = True if is_available() else False\n",
    "if iscuda: print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = CNN(len(CLASS_NAMES), 1)\n",
    "if iscuda: model_cnn = model_cnn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.print_data_shapes(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(train_loss, val_loss, val_accuracy, save_name):\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(8, 4))\n",
    "\n",
    "    \n",
    "    axes[0].plot(val_loss, marker='.', label='Validation', linewidth=.5)\n",
    "    axes[0].plot(train_loss, marker='.', label='Train', linewidth=.5)\n",
    "    axes[1].plot(val_accuracy, marker='.', label='Validation', linewidth=.5)\n",
    "\n",
    "    axes[0].legend()\n",
    "    axes[1].legend()\n",
    "\n",
    "    axes[0].set_xlabel(\"Epoch\")\n",
    "    axes[0].set_ylabel(\"Loss\")\n",
    "    \n",
    "    axes[1].set_xlabel(\"Epoch\")\n",
    "    axes[1].set_ylabel(\"Accuracy\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    fig.savefig(save_name, dpi=300, bbox_inches='tight')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, save_name, epochs=5):\n",
    "\n",
    "    class_weights = dataset_train.get_class_weights().to('cuda' if iscuda else 'cpu')\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights) # Important - class imbalance!\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    val_accuracy = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        losses = []\n",
    "        \n",
    "        for i, (inputs, labels) in tqdm(enumerate(dataloader_train, 0),desc=f\"[{epoch + 1}/{epochs}] Training\"):\n",
    "            inputs = inputs.to('cuda' if iscuda else 'cpu')\n",
    "            labels = labels.to('cuda' if iscuda else 'cpu')\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        train_loss.append(np.mean(losses))\n",
    "\n",
    "        # validation loss\n",
    "        model.eval()\n",
    "        losses = []\n",
    "        correct_preds = 0\n",
    "        \n",
    "        for i, (inputs, labels) in tqdm(enumerate(dataloader_test),desc=f\"[{epoch + 1}/{epochs}] Validation\"):\n",
    "            inputs = inputs.to('cuda' if iscuda else 'cpu')\n",
    "            labels = labels.to('cuda' if iscuda else 'cpu')\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            losses.append(loss.item())\n",
    "            correct_preds += (outputs.argmax(dim=1) == labels).float().mean().item()\n",
    "\n",
    "        val_accuracy.append(correct_preds/len(dataloader_test))\n",
    "        val_loss.append(np.mean(losses))\n",
    "        \n",
    "        # print loss\n",
    "        print(f'\\ttrain loss: {train_loss[-1]:.4f} | val loss: {val_loss[-1]:.4f}| val acc.: {val_accuracy[-1]*100.0:.2f}\\n')\n",
    "\n",
    "    torch.save(model.state_dict(), save_name)\n",
    "\n",
    "    return train_loss, val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, val_loss, val_accuracy = train(model_cnn, 'model_cnn.pth', epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(train_loss, val_loss, val_accuracy, 'model_cnn.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test(model):\n",
    "    predictions, labels = [], []\n",
    "    for i, (input, label) in tqdm(enumerate(dataloader_test),desc=f\"| Validation\"):\n",
    "        input = input.to('cuda' if iscuda else 'cpu')\n",
    "        label = label.to('cuda' if iscuda else 'cpu')\n",
    "        \n",
    "        predictions += model.predict(input).detach().cpu().numpy().tolist()\n",
    "        labels += label.detach().cpu().numpy().tolist()\n",
    "    return predictions, labels\n",
    "\n",
    "preds_cnn, labels_cnn = predict_test(model_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_cnn = classification_report(labels_cnn, preds_cnn, target_names=CLASS_NAMES, zero_division=0, digits=3)\n",
    "print(report_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_cnn = confusion_matrix(labels_cnn, preds_cnn)\n",
    "confusion_matrix_cnn = confusion_matrix_cnn / confusion_matrix_cnn[:, :].sum(axis=1)[:, np.newaxis]\n",
    "ax = heatmap(confusion_matrix_cnn, annot=True, xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES, cmap='Blues', fmt=\".3f\")\n",
    "ax.set_xlabel(\"Predicted\") \n",
    "ax.set_ylabel(\"Ground truth\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analiza cech głębokich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_conv(model):\n",
    "    features, labels = [], []\n",
    "    for i, (input, label) in tqdm(enumerate(dataloader_train),desc=f\"| Extracting features\"):\n",
    "        input = input.to('cuda' if iscuda else 'cpu')\n",
    "        label = label.to('cuda' if iscuda else 'cpu')\n",
    "        \n",
    "        features += model.get_conv_embedding(input).detach().cpu().numpy().tolist()\n",
    "        labels += label.detach().cpu().numpy().tolist()\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "def extract_features_linear(model):\n",
    "    features, labels = [], []\n",
    "    for i, (input, label) in tqdm(enumerate(dataloader_train),desc=f\"| Extracting features\"):\n",
    "        input = input.to('cuda' if iscuda else 'cpu')\n",
    "        label = label.to('cuda' if iscuda else 'cpu')\n",
    "        \n",
    "        features += model.get_fc_embedding(input).detach().cpu().numpy().tolist()\n",
    "        labels += label.detach().cpu().numpy().tolist()\n",
    "    return np.array(features), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_features(features, labels, save_name):\n",
    "        \n",
    "    tsne = TSNE(n_components=2, random_state=0, perplexity=30)\n",
    "    pca = PCA(n_components=2)\n",
    "\n",
    "    X_tsne = tsne.fit_transform(features)\n",
    "    X_pca = pca.fit_transform(features)\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        for j in range(len(CLASS_NAMES)):\n",
    "            index = np.where(labels == j)[0]\n",
    "            if i == 0:\n",
    "                ax.scatter(X_tsne[index, 0], X_tsne[index, 1], label=CLASS_NAMES[j], marker='.', color='crimson' if j == 0 else 'royalblue')\n",
    "            else:\n",
    "                ax.scatter(X_pca[index, 0], X_pca[index, 1], label=CLASS_NAMES[j], marker='.', color='crimson' if j == 0 else 'royalblue')\n",
    "        ax.legend()\n",
    "        ax.set_xlabel(f\"PC1 {pca.explained_variance_ratio_[0]*100:.2f}%\" if i == 1 else \"t-SNE 1\")\n",
    "        ax.set_ylabel(f\"PC1 {pca.explained_variance_ratio_[1]*100:.2f}%\" if i == 1 else \"t-SNE 2\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    fig.savefig(save_name, dpi=300, bbox_inches='tight')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = extract_features_conv(model_cnn)\n",
    "plot_features(features, labels, 'features_cnn_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = extract_features_linear(model_cnn)\n",
    "plot_features(features, labels, 'features_cnn_2.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
