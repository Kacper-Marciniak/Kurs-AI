{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wczytanie datasetu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "DATASET_NAME = \"FordA\"\n",
    "\n",
    "CLASS_NAMES = [\"Abnormal\", \"Normal\"]\n",
    "\n",
    "LENGTH = 500 # Długość sygnału\n",
    "\n",
    "with open(f\"{DATASET_NAME}_TRAIN.txt\", \"r\") as f:\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for line in f.readlines():\n",
    "        line = line.strip().split(\"  \")\n",
    "        class_id = int(float(line[0]))\n",
    "        class_id = max(class_id, 0)\n",
    "        data = np.array(line[1:], dtype=np.float64)[:LENGTH]\n",
    "        X_train.append(data)\n",
    "        y_train.append(class_id)\n",
    "\n",
    "with open(f\"{DATASET_NAME}_TEST.txt\", \"r\") as f:\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    for line in f.readlines():\n",
    "        line = line.strip().split(\"  \")\n",
    "        class_id = int(float(line[0]))\n",
    "        class_id = max(class_id, 0)\n",
    "        data = np.array(line[1:], dtype=np.float64)[:LENGTH]\n",
    "        X_test.append(data)\n",
    "        y_test.append(class_id)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wizualizacja przebiegów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig,axes = plt.subplots(nrows=len(CLASS_NAMES),figsize=(10,5))\n",
    "for i,ax in enumerate(axes):\n",
    "    ax.axhline(0, color='black', linestyle='-', linewidth=1)\n",
    "    for j in range(3):\n",
    "        index = np.where(y_test == i)[0][j]\n",
    "        ax.plot(np.array(X_test[index], dtype=float), linewidth=1, alpha=0.5)\n",
    "    ax.set_title(f\"{CLASS_NAMES[i]}\")\n",
    "    ax.set_xlabel(\"Samples [-]\")\n",
    "    ax.set_ylabel(\"Sensor Value [-]\")\n",
    "    ax.set_xlim(0, LENGTH)\n",
    "    ax.set_ylim(-4, 4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analiza danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_values = {}\n",
    "class_stats = {}\n",
    "for class_id in range(len(CLASS_NAMES)):\n",
    "    class_values[class_id] = []\n",
    "    class_stats[class_id] = {\n",
    "        'mean': [],\n",
    "        'mean-abs': [],\n",
    "        'std': [],\n",
    "        'std-abs': [],\n",
    "        'max': [],\n",
    "        'min': []\n",
    "    }\n",
    "    for index in np.where(y_train == class_id)[0]:\n",
    "        class_values[class_id] += X_train[index].tolist()\n",
    "\n",
    "        class_stats[class_id]['mean'].append(np.mean(X_train[index]))\n",
    "        class_stats[class_id]['mean-abs'].append(np.mean(np.abs(X_train[index])))\n",
    "        class_stats[class_id]['std'].append(np.std(X_train[index]))\n",
    "        class_stats[class_id]['std-abs'].append(np.std(np.abs(X_train[index])))\n",
    "        class_stats[class_id]['max'].append(np.max(X_train[index]))\n",
    "        class_stats[class_id]['min'].append(np.min(X_train[index]))\n",
    "    for key in class_stats[class_id]:\n",
    "        class_stats[class_id][key] = np.mean(class_stats[class_id][key]), np.std(class_stats[class_id][key])\n",
    "\n",
    "fig,axes = plt.subplots(nrows=len(CLASS_NAMES),figsize=(6,7))\n",
    "\n",
    "for i,ax in enumerate(axes):\n",
    "    ax.hist(class_values[i], bins=50, alpha=0.75, color='royalblue', edgecolor='black')\n",
    "    ax.set_title(f\"{CLASS_NAMES[i]}\")\n",
    "    ax.set_xlabel(\"Sensor Value [-]\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for class_id in range(len(CLASS_NAMES)):\n",
    "    print(f\"Class: {CLASS_NAMES[class_id]}\")\n",
    "    print(f\"\\tMean: {class_stats[class_id]['mean'][0]:.3f} ({class_stats[class_id]['mean'][1]:.3f})\")\n",
    "    print(f\"\\tStd: {class_stats[class_id]['std'][0]:.3f} ({class_stats[class_id]['std'][1]:.3f})\")\n",
    "    print(f\"\\tMean Abs: {class_stats[class_id]['mean-abs'][0]:.3f} ({class_stats[class_id]['mean-abs'][1]:.3f})\")\n",
    "    print(f\"\\tStd Abs: {class_stats[class_id]['std-abs'][0]:.3f} ({class_stats[class_id]['std-abs'][1]:.3f})\")\n",
    "    print(f\"\\tMax: {class_stats[class_id]['max'][0]:.3f} ({class_stats[class_id]['max'][1]:.3f})\")\n",
    "    print(f\"\\tMin: {class_stats[class_id]['min'][0]:.3f} ({class_stats[class_id]['min'][1]:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analiza w domenie częstotliwościowej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fft_data(data: np.ndarray) -> np.ndarray:\n",
    "    fft = np.fft.fft(np.array(data, dtype=float))\n",
    "    freq = np.fft.fftfreq(fft.shape[-1])\n",
    "    fft = fft[freq >= 0]\n",
    "    freq = freq[freq >= 0]\n",
    "    amplitude = (fft.real**2 + fft.imag**2)**.5\n",
    "    phase = np.arctan2(fft.imag, fft.real)\n",
    "    return freq, amplitude, phase\n",
    "\n",
    "def transform_to_freq_domain(data: np.ndarray) -> np.ndarray:\n",
    "    return np.array([get_fft_data(d)[1] for d in data], dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig,axes = plt.subplots(nrows=len(CLASS_NAMES),ncols=2,figsize=(10,5))\n",
    "for i in range(len(axes)):\n",
    "    for j in range(5):\n",
    "        index = np.where(y_train == i)[0][j]\n",
    "        freq, amplitude, phase = get_fft_data(X_train[index])\n",
    "        \n",
    "        \n",
    "        axes[i][0].plot(freq, amplitude, linewidth=1, alpha=0.5)\n",
    "        axes[i][1].plot(freq, phase, linewidth=1, alpha=0.5)\n",
    "    for j in range(2):\n",
    "        axes[i][j].set_title(f\"{CLASS_NAMES[i]}\")\n",
    "        axes[i][j].set_xlabel(\"Frequency [-]\")\n",
    "        axes[i][j].set_xlim(0, freq.max())\n",
    "    axes[i][0].set_ylabel(\"Amplitude [-]\")\n",
    "    axes[i][1].set_ylabel(\"Phase [-]\")\n",
    "    #ax.set_ylim(-4, 4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W toku zajęć wykorzystamy dwa zestawy danych wejściowych:\n",
    "- dane w domenie czasowej (przebieg czasowy)\n",
    "- dane w domenie częstotliwościowej (amplitudy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analiza cech wejściowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Macierz korelacji w domenie czasowej\n",
    "\n",
    "X_train_corr = np.corrcoef(X_train.T)\n",
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(X_train_corr, ax=ax, cmap='coolwarm', center=0)\n",
    "ax.set_title(\"Correlation Matrix\")\n",
    "ax.set_xlabel(\"Samples [-]\", fontdict={'fontsize': 9})\n",
    "ax.set_ylabel(\"Samples [-]\", fontdict={'fontsize': 9})\n",
    "plt.show()\n",
    "\n",
    "# Macierz korelacji w domenie czasowej\n",
    "\n",
    "X_train_corr = np.corrcoef(transform_to_freq_domain(X_train).T)\n",
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(X_train_corr, ax=ax, cmap='coolwarm', center=0)\n",
    "ax.set_title(\"Correlation Matrix\")\n",
    "ax.set_xlabel(\"Samples [-]\", fontdict={'fontsize': 9})\n",
    "ax.set_ylabel(\"Samples [-]\", fontdict={'fontsize': 9})\n",
    "plt.show()\n",
    "\n",
    "del X_train_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zmienność (odchylenie) cech wejściowych\n",
    "\n",
    "fig,axes = plt.subplots(figsize=(12,4), ncols=2)\n",
    "axes[0].plot(np.std(X_train[y_train==0], axis=0), linewidth=1, alpha=0.75)\n",
    "axes[0].plot(np.std(X_train[y_train==1], axis=0), linewidth=1, alpha=0.75)\n",
    "axes[0].set_title(\"Input Features Variance (Time Domain)\")\n",
    "axes[1].plot(np.mean(X_train[y_train==0], axis=0), linewidth=1, alpha=0.75)\n",
    "axes[1].plot(np.mean(X_train[y_train==1], axis=0), linewidth=1, alpha=0.75)\n",
    "axes[1].set_title(\"Input Features Mean Value (Time Domain)\")\n",
    "plt.show()\n",
    "\n",
    "X_train_fft = transform_to_freq_domain(X_train)\n",
    "fig,axes = plt.subplots(figsize=(12,4), ncols=2)\n",
    "axes[0].plot(np.std(X_train_fft[y_train==0], axis=0), linewidth=1, alpha=0.75)\n",
    "axes[0].plot(np.std(X_train_fft[y_train==1], axis=0), linewidth=1, alpha=0.75)\n",
    "axes[0].set_title(\"Input Features Variance (Frequency Domain)\")\n",
    "axes[1].plot(np.mean(X_train_fft[y_train==0], axis=0), linewidth=1, alpha=0.75)\n",
    "axes[1].plot(np.mean(X_train_fft[y_train==1], axis=0), linewidth=1, alpha=0.75)\n",
    "axes[1].set_title(\"Input Features Mean Value (Frequency Domain)\")\n",
    "plt.show()\n",
    "\n",
    "del X_train_fft\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Klasyfikacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "fft_features = FunctionTransformer(transform_to_freq_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stworzenie pipeline'u\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "USE_FFT = True\n",
    "\n",
    "if USE_FFT:\n",
    "    pipeline_SVM = Pipeline([\n",
    "        ('feature_extractor', fft_features),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', SVC(kernel='rbf'))\n",
    "    ])\n",
    "\n",
    "    pipeline_RF = Pipeline([\n",
    "        ('feature_extractor', fft_features),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "    pipeline_KNN = Pipeline([\n",
    "        ('feature_extractor', fft_features),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', KNeighborsClassifier())\n",
    "    ])\n",
    "else:\n",
    "    pipeline_SVM = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', SVC(kernel='rbf'))\n",
    "    ])\n",
    "\n",
    "    pipeline_RF = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "    pipeline_KNN = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', KNeighborsClassifier())\n",
    "    ])\n",
    "\n",
    "# Fit\n",
    "\n",
    "pipeline_SVM.fit(X_train, y_train)\n",
    "pipeline_RF.fit(X_train, y_train)\n",
    "pipeline_KNN.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from seaborn import heatmap\n",
    "\n",
    "for pipeline, name in zip((pipeline_SVM, pipeline_RF, pipeline_KNN), (\"SVM\", \"RandomForest\", \"KNN\")):\n",
    "\n",
    "    preds = pipeline.predict(X_test)\n",
    "\n",
    "    report = classification_report(y_test, preds, target_names=CLASS_NAMES, zero_division=0, digits=3)\n",
    "    print(report)\n",
    "\n",
    "    confusion_matrix_ = confusion_matrix(y_test, preds)\n",
    "    confusion_matrix_ = confusion_matrix_ / confusion_matrix_[:, :].sum(axis=1)[:, np.newaxis]\n",
    "    ax = heatmap(confusion_matrix_, annot=True, xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES, cmap='Blues', fmt=\".3f\")\n",
    "    ax.set_xlabel(\"Predicted\") \n",
    "    ax.set_ylabel(\"Ground truth\")\n",
    "    ax.set_title(f\"{name}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wykorzystanie CNN oraz MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stworzenie datasetu i dataloadera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray, use_fft: bool):\n",
    "        if use_fft:\n",
    "            # Transform to frequency domain\n",
    "            X = transform_to_freq_domain(X)\n",
    "        else:\n",
    "            # Add channel dimension\n",
    "            X = np.expand_dims(X, axis=1) # N x C x L, with C=1\n",
    "        \n",
    "        # Standarize\n",
    "        X = (X - np.mean(X)) / np.std(X)\n",
    "\n",
    "        self.X = torch.tensor(X.astype(np.float32))\n",
    "        self.y = torch.tensor(y.astype(np.int64))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "    def get_class_weights(self):\n",
    "        class_sample_count = np.unique(self.y.numpy(), return_counts=True)[1]\n",
    "        weight = (np.sum(class_sample_count) / class_sample_count).astype(np.float32)\n",
    "        return torch.from_numpy(weight)\n",
    "\n",
    "\n",
    "# Dataloaders\n",
    "dataset_train = CustomDataset(X_train, y_train, use_fft=False)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "dataset_test = CustomDataset(X_test, y_test, use_fft=False)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "dataset_train_fft = CustomDataset(X_train, y_train, use_fft=True)\n",
    "dataloader_train_fft = DataLoader(dataset_train_fft, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "dataset_test_fft = CustomDataset(X_test, y_test, use_fft=True)\n",
    "dataloader_test_fft = DataLoader(dataset_test_fft, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(dataset_train.get_class_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definicja własnej sieci neuronowej\n",
    "\n",
    "https://journals.sagepub.com/doi/full/10.3233/ICA-200617"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_class: int, n_channels_in: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        CHANNELS_0 = 32\n",
    "        CHANNELS_1 = 64\n",
    "        CHANNELS_2 = 128\n",
    "        NEURONS_0 = 512\n",
    "        NEURONS_1 = 128\n",
    "\n",
    "        self.conv0 = nn.Sequential(\n",
    "            nn.Conv1d(n_channels_in, out_channels=CHANNELS_0, kernel_size=7, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(CHANNELS_0, out_channels=CHANNELS_1, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(CHANNELS_1, out_channels=CHANNELS_2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "        )\n",
    "        \n",
    "        self.fc0 = nn.Sequential(\n",
    "            nn.Linear(CHANNELS_2, NEURONS_0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(NEURONS_0, NEURONS_1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(NEURONS_1, n_class),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        out = self.conv0(x)\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        # Global max pooling\n",
    "        out = torch.max(out, dim=2).values\n",
    "\n",
    "        out = self.fc0(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def print_data_shapes(self, input_length: int) -> None:\n",
    "\n",
    "        x = torch.randn(1, 1, input_length).to(next(self.parameters()).device)\n",
    "        print(f\"Input shape: {x.shape[1:]}\")\n",
    "\n",
    "        out = self.conv0(x)\n",
    "        print(f\"Conv0 shape: {out.shape[1:]}\")\n",
    "\n",
    "        out = self.conv1(out)\n",
    "        print(f\"Conv1 shape: {out.shape[1:]}\")\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        print(f\"Conv2 shape: {out.shape[1:]}\")\n",
    "\n",
    "        out = torch.max(out, dim=2).values\n",
    "        print(f\"Global max pooling shape: {out.shape}\")\n",
    "\n",
    "        out = self.fc0(out)\n",
    "        print(f\"FC0 shape: {out.shape[1:]}\")\n",
    "\n",
    "        out = self.fc1(out)\n",
    "        print(f\"FC1 shape: {out.shape[1:]}\")\n",
    "\n",
    "        out = self.fc2(out)\n",
    "        print(f\"FC2 shape: {out.shape[1:]}\")\n",
    "    \n",
    "    def get_conv_embedding(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        out = self.conv0(x)\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        # Global max pooling\n",
    "        out = torch.max(out, dim=2).values\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def get_fc_embedding(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.conv0(x)\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        # Global max pooling\n",
    "        out = torch.max(out, dim=2).values\n",
    "\n",
    "\n",
    "        out = self.fc0(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc1(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = self(X).argmax(dim=1)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_class: int, n_features_in: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        NEURONS_0 = n_features_in\n",
    "        NEURONS_1 = NEURONS_0*4\n",
    "        NEURONS_2 = NEURONS_0\n",
    "        \n",
    "        self.fc0 = nn.Sequential(\n",
    "            nn.Linear(NEURONS_0, NEURONS_1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(NEURONS_1, NEURONS_2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(NEURONS_2, n_class),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        out = self.fc0(x)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "                \n",
    "        return out\n",
    "    \n",
    "    def get_fc_embedding(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        out = self.fc0(x)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc1(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = self(X).argmax(dim=1)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda import is_available\n",
    "\n",
    "iscuda = True if is_available() else False\n",
    "if iscuda: print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = CNN(len(CLASS_NAMES), 1)\n",
    "model_mlp = MLP(len(CLASS_NAMES), LENGTH//2)\n",
    "if iscuda: \n",
    "    model_cnn = model_cnn.cuda()\n",
    "    model_mlp = model_mlp.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.print_data_shapes(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training DL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(train_loss, val_loss, val_accuracy, save_name):\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(8, 4))\n",
    "\n",
    "    \n",
    "    axes[0].plot(val_loss, marker='.', label='Validation', linewidth=.5)\n",
    "    axes[0].plot(train_loss, marker='.', label='Train', linewidth=.5)\n",
    "    axes[1].plot(val_accuracy, marker='.', label='Validation', linewidth=.5)\n",
    "\n",
    "    axes[0].legend()\n",
    "    axes[1].legend()\n",
    "\n",
    "    axes[0].set_xlabel(\"Epoch\")\n",
    "    axes[0].set_ylabel(\"Loss\")\n",
    "    \n",
    "    axes[1].set_xlabel(\"Epoch\")\n",
    "    axes[1].set_ylabel(\"Accuracy\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    fig.savefig(save_name, dpi=300, bbox_inches='tight')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(dataloader_train, dataloader_test, model, save_name, epochs=5):\n",
    "\n",
    "    class_weights = dataset_train.get_class_weights().to('cuda' if iscuda else 'cpu')\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights) # Important - class imbalance!\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    val_accuracy = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        losses = []\n",
    "        \n",
    "        for i, (inputs, labels) in tqdm(enumerate(dataloader_train, 0),desc=f\"[{epoch + 1}/{epochs}] Training\"):\n",
    "            inputs = inputs.to('cuda' if iscuda else 'cpu')\n",
    "            labels = labels.to('cuda' if iscuda else 'cpu')\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        train_loss.append(np.mean(losses))\n",
    "\n",
    "        # validation loss\n",
    "        model.eval()\n",
    "        losses = []\n",
    "        correct_preds = 0\n",
    "        \n",
    "        for i, (inputs, labels) in tqdm(enumerate(dataloader_test),desc=f\"[{epoch + 1}/{epochs}] Validation\"):\n",
    "            inputs = inputs.to('cuda' if iscuda else 'cpu')\n",
    "            labels = labels.to('cuda' if iscuda else 'cpu')\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            losses.append(loss.item())\n",
    "            correct_preds += (outputs.argmax(dim=1) == labels).float().mean().item()\n",
    "\n",
    "        val_accuracy.append(correct_preds/len(dataloader_test))\n",
    "        val_loss.append(np.mean(losses))\n",
    "        \n",
    "        # print loss\n",
    "        print(f'\\ttrain loss: {train_loss[-1]:.4f} | val loss: {val_loss[-1]:.4f}| val acc.: {val_accuracy[-1]*100.0:.2f}\\n')\n",
    "\n",
    "    torch.save(model.state_dict(), save_name)\n",
    "\n",
    "    return train_loss, val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, val_loss, val_accuracy = train(dataloader_train, dataloader_test, model_cnn, 'model_cnn.pth', epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(train_loss, val_loss, val_accuracy, 'model_cnn.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, val_loss, val_accuracy = train(dataloader_train_fft, dataloader_test_fft, model_mlp, 'model_mlp.pth', epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(train_loss, val_loss, val_accuracy, 'model_mlp.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test(model, dataloader_test):\n",
    "    predictions, labels = [], []\n",
    "    for i, (input, label) in tqdm(enumerate(dataloader_test),desc=f\"| Validation\"):\n",
    "        input = input.to('cuda' if iscuda else 'cpu')\n",
    "        label = label.to('cuda' if iscuda else 'cpu')\n",
    "        \n",
    "        predictions += model.predict(input).detach().cpu().numpy().tolist()\n",
    "        labels += label.detach().cpu().numpy().tolist()\n",
    "    return predictions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_cnn, labels_cnn = predict_test(model_cnn, dataloader_test)\n",
    "\n",
    "report_cnn = classification_report(labels_cnn, preds_cnn, target_names=CLASS_NAMES, zero_division=0, digits=3)\n",
    "print(report_cnn)\n",
    "\n",
    "confusion_matrix_cnn = confusion_matrix(labels_cnn, preds_cnn)\n",
    "confusion_matrix_cnn = confusion_matrix_cnn / confusion_matrix_cnn[:, :].sum(axis=1)[:, np.newaxis]\n",
    "ax = heatmap(confusion_matrix_cnn, annot=True, xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES, cmap='Blues', fmt=\".3f\")\n",
    "ax.set_xlabel(\"Predicted\") \n",
    "ax.set_ylabel(\"Ground truth\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mlp, labels_mlp = predict_test(model_mlp, dataloader_test_fft)\n",
    "\n",
    "report_mlp = classification_report(labels_mlp, preds_mlp, target_names=CLASS_NAMES, zero_division=0, digits=3)\n",
    "print(report_mlp)\n",
    "\n",
    "confusion_matrix_mlp = confusion_matrix(labels_mlp, preds_mlp)\n",
    "confusion_matrix_mlp = confusion_matrix_mlp / confusion_matrix_mlp[:, :].sum(axis=1)[:, np.newaxis]\n",
    "ax = heatmap(confusion_matrix_mlp, annot=True, xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES, cmap='Blues', fmt=\".3f\")\n",
    "ax.set_xlabel(\"Predicted\") \n",
    "ax.set_ylabel(\"Ground truth\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analiza cech głębokich - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_conv(model, dataloader_test):\n",
    "    features, labels = [], []\n",
    "    for i, (input, label) in tqdm(enumerate(dataloader_test),desc=f\"| Extracting features\"):\n",
    "        input = input.to('cuda' if iscuda else 'cpu')\n",
    "        label = label.to('cuda' if iscuda else 'cpu')\n",
    "        \n",
    "        features += model.get_conv_embedding(input).detach().cpu().numpy().tolist()\n",
    "        labels += label.detach().cpu().numpy().tolist()\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "def extract_features_linear(model, dataloader_test):\n",
    "    features, labels = [], []\n",
    "    for i, (input, label) in tqdm(enumerate(dataloader_test),desc=f\"| Extracting features\"):\n",
    "        input = input.to('cuda' if iscuda else 'cpu')\n",
    "        label = label.to('cuda' if iscuda else 'cpu')\n",
    "        \n",
    "        features += model.get_fc_embedding(input).detach().cpu().numpy().tolist()\n",
    "        labels += label.detach().cpu().numpy().tolist()\n",
    "    return np.array(features), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_features(features, labels, save_name):\n",
    "        \n",
    "    tsne = TSNE(n_components=2, random_state=0, perplexity=40)\n",
    "    pca = PCA(n_components=2)\n",
    "\n",
    "    X_tsne = tsne.fit_transform(features)\n",
    "    X_pca = pca.fit_transform(features)\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        for j in range(len(CLASS_NAMES)):\n",
    "            index = np.where(labels == j)[0]\n",
    "            if i == 0:\n",
    "                ax.scatter(X_tsne[index, 0], X_tsne[index, 1], label=CLASS_NAMES[j], marker='.', color='crimson' if j == 0 else 'royalblue')\n",
    "            else:\n",
    "                ax.scatter(X_pca[index, 0], X_pca[index, 1], label=CLASS_NAMES[j], marker='.', color='crimson' if j == 0 else 'royalblue')\n",
    "        ax.legend()\n",
    "        ax.set_xlabel(f\"PC1 {pca.explained_variance_ratio_[0]*100:.2f}%\" if i == 1 else \"t-SNE 1\")\n",
    "        ax.set_ylabel(f\"PC1 {pca.explained_variance_ratio_[1]*100:.2f}%\" if i == 1 else \"t-SNE 2\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    fig.savefig(save_name, dpi=300, bbox_inches='tight')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = extract_features_conv(model_cnn, dataloader_test)\n",
    "plot_features(features, labels, 'features_cnn_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = extract_features_linear(model_cnn, dataloader_test)\n",
    "plot_features(features, labels, 'features_cnn_2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = extract_features_linear(model_mlp, dataloader_test_fft)\n",
    "plot_features(features, labels, 'features_cnn_2.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
