{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pobranie datasetu MVTEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mvtec import MVTecSingleClassDataset, CLASS_NAMES\n",
    "import os\n",
    "\n",
    "print(\"MVTec-AD classes:\")\n",
    "for i,class_name in enumerate(CLASS_NAMES):\n",
    "    print(f\"{i+1}. {class_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wybór klasy z datasetu MVTec-AD\n",
    "CLASS = \"bottle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pobieranie jedynie wybranej klasy z datasetu\n",
    "\n",
    "DATASET_PATH = os.path.abspath(\"dataset\")\n",
    "dataset = MVTecSingleClassDataset(dataset_path=DATASET_PATH, class_name=CLASS, subset=\"train\")\n",
    "\n",
    "print(f\"Number of images: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przegląd danych\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import cv2 as cv\n",
    "from mvtec import TRANSFORM\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def get_transform_visualisation(image):\n",
    "    image = Image.fromarray(image)\n",
    "    image = TRANSFORM(image).numpy()\n",
    "    image = np.moveaxis(image, 0, -1)\n",
    "    # Normalize to 0->1\n",
    "    image = (image - image.min()) / (image.max() - image.min())\n",
    "\n",
    "    return image\n",
    "\n",
    "for anomaly_type in os.listdir(f\"{DATASET_PATH}/{CLASS}/test\"):\n",
    "    im = glob.glob(f\"{DATASET_PATH}/{CLASS}/test/{anomaly_type}/*.png\")[0]\n",
    "    image = cv.imread(im)\n",
    "    mask_path = f\"{DATASET_PATH}/{CLASS}/ground_truth/{anomaly_type}/{os.path.basename(im).split('.')[0]}_mask.png\"\n",
    "    \n",
    "    \n",
    "    if os.path.exists(mask_path):\n",
    "        mask = cv.imread(mask_path,cv.IMREAD_GRAYSCALE)\n",
    "        contour = cv.findContours(mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)[0]\n",
    "        cv.drawContours(image, contour, -1, (255, 0, 0), 2)\n",
    "    \n",
    "    image_transformed = get_transform_visualisation(image)\n",
    "    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "    image_transformed = cv.cvtColor(image_transformed, cv.COLOR_BGR2RGB)\n",
    "    \n",
    "    fig,axes = plt.subplots(1,2,figsize=(10,5))\n",
    "    \n",
    "    axes[0].imshow(image)\n",
    "    axes[1].imshow(image_transformed)\n",
    "\n",
    "    axes[0].set_title(anomaly_type.capitalize())\n",
    "    axes[1].set_title(anomaly_type.capitalize()+\" (transformed)\")\n",
    "\n",
    "    axes[0].axis(\"off\")\n",
    "    axes[1].axis(\"off\")\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definicja subsetów train oraz test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MVTecSingleClassDataset(DATASET_PATH, class_name=CLASS, subset='train')\n",
    "test_dataset = MVTecSingleClassDataset(DATASET_PATH, class_name=CLASS, subset='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utworzenie instancji modelu PADIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from padim import Padim\n",
    "import numpy as np\n",
    "\n",
    "ARCHITECTURE = 'wide_resnet50_2' #'resnet18'\n",
    "\n",
    "padim = Padim(arch=ARCHITECTURE, save_path='results', classname=CLASS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ekstrakcja cech głębokich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f'results/embedding_{train_dataset.class_name}.npz'):\n",
    "    embedding_vectors_test, anomaly_types_test = padim.extract_features(test_dataset)\n",
    "\n",
    "    embedding_vectors_test = embedding_vectors_test.cpu().numpy()\n",
    "\n",
    "    np.savez_compressed(f'results/embedding_{test_dataset.class_name}.npz', embedding_vectors=embedding_vectors_test, anomaly_types=anomaly_types_test)\n",
    "else:\n",
    "    data = np.load(f'results/embedding_{test_dataset.class_name}.npz')\n",
    "    embedding_vectors_test = data[\"embedding_vectors\"]\n",
    "    anomaly_types_test = data[\"anomaly_types\"]\n",
    "\n",
    "    del data\n",
    "\n",
    "\n",
    "print(embedding_vectors_test.shape)\n",
    "embedding_vectors_test = np.max(embedding_vectors_test, axis=(2,3))\n",
    "print(embedding_vectors_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "embedding_vectors_tsne = tsne.fit_transform(embedding_vectors_test)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(embedding_vectors_tsne[anomaly_types_test == \"good\", 0], embedding_vectors_tsne[anomaly_types_test == \"good\", 1], label=\"good\", marker='.', color='black')\n",
    "for anomaly_type in np.unique(anomaly_types_test):\n",
    "    if anomaly_type == \"good\": continue\n",
    "    plt.scatter(embedding_vectors_tsne[anomaly_types_test == anomaly_type, 0], embedding_vectors_tsne[anomaly_types_test == anomaly_type, 1], label=anomaly_type, marker='x')\n",
    "plt.legend()\n",
    "plt.title('TSNE')\n",
    "plt.xlabel('TSNE 1')\n",
    "plt.ylabel('TSNE 2')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "del embedding_vectors_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2, random_state=0)\n",
    "embedding_vectors_pca = pca.fit_transform(embedding_vectors_test)\n",
    "pca.explained_variance_ratio_\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(embedding_vectors_pca[anomaly_types_test == \"good\", 0], embedding_vectors_pca[anomaly_types_test == \"good\", 1], label=\"good\", marker='.', color='black')\n",
    "for anomaly_type in np.unique(anomaly_types_test):\n",
    "    if anomaly_type == \"good\": continue\n",
    "    plt.scatter(embedding_vectors_pca[anomaly_types_test == anomaly_type, 0], embedding_vectors_pca[anomaly_types_test == anomaly_type, 1], label=anomaly_type, marker='x')\n",
    "plt.legend()\n",
    "plt.title('PCA')\n",
    "plt.xlabel('PC 1: {:.2f}%'.format(pca.explained_variance_ratio_[0]*100))\n",
    "plt.ylabel('PC 2: {:.2f}%'.format(pca.explained_variance_ratio_[1]*100))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "del embedding_vectors_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trening i walidacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padim.train(train_dataset=train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padim.validate(test_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2 as cv\n",
    "from time import time\n",
    "import random\n",
    "\n",
    "image_sample = random.sample(glob.glob(f'{DATASET_PATH}\\\\{CLASS}\\\\test\\\\*\\\\*.png'), k=5)\n",
    "\n",
    "for im_path in image_sample:\n",
    "    print(im_path)\n",
    "\n",
    "    image = cv.imread(im_path)\n",
    "    #image = modify_image(image)\n",
    "    \n",
    "    class_type = im_path.split(\"\\\\\")[-2]\n",
    "    processing_time = time()\n",
    "    mask, heatmap = padim.preview(image)\n",
    "    processing_time = time() - processing_time\n",
    "\n",
    "    preview = image.copy()\n",
    "\n",
    "\n",
    "    mask = cv.resize(mask, (image.shape[1], image.shape[0]), cv.INTER_NEAREST_EXACT).astype(np.uint8)\n",
    "    heatmap = cv.resize(heatmap, (image.shape[1], image.shape[0]), cv.INTER_CUBIC).astype(np.uint8)    \n",
    "    heatmap = cv.applyColorMap(heatmap, cv.COLORMAP_JET)\n",
    "\n",
    "    preview = cv.addWeighted(heatmap, 0.5, preview, 0.5, 0)\n",
    "    contours = cv.findContours(mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)[0]\n",
    "    \n",
    "    cv.drawContours(preview, contours, -1, (0,255,0), 2)\n",
    "\n",
    "    mask_path = f\"{os.path.dirname(os.path.dirname(os.path.dirname(im_path)))}\\\\ground_truth\\\\{os.path.basename(os.path.dirname(im_path))}\\\\{os.path.basename(im_path).split('.')[0]}_mask.png\"\n",
    "    if os.path.exists(mask_path):\n",
    "        mask_gt = cv.imread(mask_path)\n",
    "    else:\n",
    "        mask_gt = np.zeros_like(image)\n",
    "    cv.drawContours(mask_gt, contours, -1, (255,0,0), 2)\n",
    "    fig,axes = plt.subplots(1,3,figsize=(15,5))\n",
    "    axes[0].imshow(cv.cvtColor(image, cv.COLOR_BGR2RGB))\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    axes[1].imshow(cv.cvtColor(preview, cv.COLOR_BGR2RGB))\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    axes[2].imshow(mask_gt, cmap='gray')\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    fig.suptitle(f\"{class_type.capitalize()} - {os.path.basename(im_path)}. Processing time: {processing_time:.1f} s\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modyfikacja zdjęć wejściowych - analiza odporności modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_image(image: np.ndarray) -> np.ndarray:\n",
    "    return (image*.50).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = glob.glob(f'{DATASET_PATH}\\\\{CLASS}\\\\test\\\\*\\\\*.png')[0]\n",
    "\n",
    "image = cv.imread(image)\n",
    "image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "image_mod = modify_image(image)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axes[0].imshow(image)\n",
    "axes[1].imshow(image_mod)\n",
    "axes[0].set_title('Original')\n",
    "axes[1].set_title('Modified')\n",
    "axes[0].axis('off')\n",
    "axes[1].axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2 as cv\n",
    "import os\n",
    "import sys\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "\n",
    "\n",
    "def get_predictions(modify: bool = False):\n",
    "\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    for im_path in glob.glob(f'{DATASET_PATH}\\\\{CLASS}\\\\test\\\\*\\\\*.png'):\n",
    "        print(im_path)\n",
    "\n",
    "        image = cv.imread(im_path)\n",
    "        class_type = im_path.split(\"\\\\\")[-2]\n",
    "\n",
    "        if modify:\n",
    "            image = modify_image(image)\n",
    "\n",
    "        mask, _ = padim.preview(image)\n",
    "        pred_is_anomaly = np.count_nonzero(mask) > 0\n",
    "        label_is_anomaly = class_type != \"good\"\n",
    "\n",
    "        predictions.append(pred_is_anomaly)\n",
    "        labels.append(label_is_anomaly)\n",
    "\n",
    "    return labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels1, predictions1 = get_predictions(modify=False)\n",
    "labels2, predictions2 = get_predictions(modify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = recall_score(labels1, predictions1)\n",
    "precision = precision_score(labels1, predictions1)\n",
    "f1 = f1_score(labels2, predictions2)\n",
    "\n",
    "print(\"\\nWithout modification\\n\")\n",
    "\n",
    "print(f\"All images: {len(labels1)}\")\n",
    "print(f\"Images with anomaly: {sum(labels1)}\")\n",
    "\n",
    "print(f\"Recall: {recall*100.0:.2f}\")\n",
    "print(f\"Precision: {precision*100.0:.2f}\")\n",
    "print(f\"F1: {f1*100.0:.2f}\")\n",
    "\n",
    "recall = recall_score(labels2, predictions2)\n",
    "precision = precision_score(labels2, predictions2)\n",
    "f1 = f1_score(labels2, predictions2)\n",
    "\n",
    "print(\"\\nWith modification\\n\")\n",
    "\n",
    "print(f\"All images: {len(labels2)}\")\n",
    "print(f\"Images with anomaly: {sum(labels2)}\")\n",
    "\n",
    "print(f\"Recall: {recall*100.0:.2f}\")\n",
    "print(f\"Precision: {precision*100.0:.2f}\")\n",
    "print(f\"F1: {f1*100.0:.2f}\")\n",
    "\n",
    "# Walidację wykonaliśmy jak dla zadania klasyfikacji binarnej (wykrywanie anomalii)\n",
    "# Wyniki nie mają obiektywnego charakteru, gdyż zestaw danych nie jest ZBALANSOWANY\n",
    "# W zbiorze testowym jest zdecydowanie więcej obrazów z anomaliami niż bez nich\n",
    "# W związku z tym, model uzyska wysokie wyniki, nawet gdy zawsze będzie przewidywał obecność anomalii!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyznaczmy metryki jak dla zadania multi-klasowego (macro)\n",
    "\n",
    "recall = recall_score(labels1, predictions1, average='macro')\n",
    "precision = precision_score(labels1, predictions1, average='macro', zero_division=0)\n",
    "f1 = f1_score(labels2, predictions2, average='macro', zero_division=0)\n",
    "\n",
    "print(\"\\nWithout modification\\n\")\n",
    "\n",
    "print(f\"All images: {len(labels1)}\")\n",
    "print(f\"Images with anomaly: {sum(labels1)}\")\n",
    "\n",
    "print(f\"Recall: {recall*100.0:.2f}\")\n",
    "print(f\"Precision: {precision*100.0:.2f}\")\n",
    "print(f\"F1: {f1*100.0:.2f}\")\n",
    "\n",
    "recall = recall_score(labels2, predictions2, average='macro')\n",
    "precision = precision_score(labels2, predictions2, average='macro', zero_division=0)\n",
    "f1 = f1_score(labels2, predictions2, average='macro', zero_division=0)\n",
    "\n",
    "print(\"\\nWith modification\\n\")\n",
    "\n",
    "print(f\"All images: {len(labels2)}\")\n",
    "print(f\"Images with anomaly: {sum(labels2)}\")\n",
    "\n",
    "print(f\"Recall: {recall*100.0:.2f}\")\n",
    "print(f\"Precision: {precision*100.0:.2f}\")\n",
    "print(f\"F1: {f1*100.0:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
